{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e00f269d",
      "metadata": {},
      "source": [
        "# SemanticChunker\n",
        "\n",
        "- Author: [Wonyoung Lee](https://github.com/BaBetterB)\n",
        "- Design: []()\n",
        "- Peer Review : [Wooseok Jeong](https://github.com/jeong-wooseok), [sohyunwriter](https://github.com/sohyunwriter)\n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/07-TextSplitter/04-SemanticChunker.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/07-TextSplitter/04-SemanticChunker.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers a Text Splitter that splits text based on semantic similarity.\n",
        "\n",
        "The **Semantic Chunker** is a sophisticated tool within **LangChain** that brings an intelligent approach to document chunking. Rather than simply dividing text at fixed intervals, it analyzes the semantic meaning of content to create more meaningful divisions. \n",
        "\n",
        "This process relies on **OpenAI's embedding model** , which evaluates how similar different pieces of text are to each other. The tool offers flexible splitting options, including percentile-based, standard deviation, and interquartile range methods. \n",
        "\n",
        "What sets it apart from traditional text splitters is its ability to maintain context by identifying natural break points in the text, ultimately leading to better performance when working with large language models. \n",
        "\n",
        "By understanding the actual meaning of the content, it creates more coherent and useful chunks that preserve the original document's context and flow.\n",
        "\n",
        " [Greg Kamradt's Notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
        "\n",
        "The method divides the text into sentence units, then groups them into three sentences, and merges similar sentences in the embedding space.\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environement Setup](#environment-setup)\n",
        "- [Creating a Semantic Chunker](#creating-a-semanticchunker)\n",
        "- [Text Splitting](#text-splitting)\n",
        "- [Breakpoints](#breakpoints)\n",
        "\n",
        "### References\n",
        "\n",
        "- [Greg Kamradt's Notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
        "\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b06978",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b48cce",
      "metadata": {},
      "source": [
        "Load sample text and output the content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3a8858",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e18ec26d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain-anthropic\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_text_splitters\",\n",
        "        \"langchain_openai\",\n",
        "        \"langchain_experimental\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a1683c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"SemanticChunker\",  # title\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54038451",
      "metadata": {},
      "source": [
        "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it.\n",
        "\n",
        "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295e7ad6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration File for Managing API Keys as Environment Variables\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API Key Information\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c170dd43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open the data/appendix-keywords.txt file to create a file object called f.\n",
        "with open(\"./data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
        "\n",
        "    file = f.read()  # Read the contents of the file and save it in the file variable.\n",
        "\n",
        "# Print part of the content read from the file.\n",
        "print(file[:350])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d165846",
      "metadata": {},
      "source": [
        "## Creating a SemanticChunker\n",
        "\n",
        "`SemanticChunker` is one of LangChain's experimental features, which serves to divide text into semantically similar chunks.\n",
        "\n",
        "This allows you to process and analyze text data more effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab33ae70",
      "metadata": {},
      "source": [
        "Use `SemanticChunker` to divide the text into semantically related chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312e3aae",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Initialize a semantic chunk splitter using OpenAI embeddings.\n",
        "text_splitter = SemanticChunker(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab515b0",
      "metadata": {},
      "source": [
        "## Text Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c9b20b",
      "metadata": {},
      "source": [
        "- Use `text_splitter` to divide the `file` text into document units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb5870d",
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_text(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a777bc",
      "metadata": {},
      "source": [
        "Check the divided chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec69bff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first chunk among the divided chunks.\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f03b26b",
      "metadata": {},
      "source": [
        "You can convert chunks to documents using the `create_documents()` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fadaf823",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split using text_splitter\n",
        "docs = text_splitter.create_documents([file])\n",
        "print(\n",
        "    docs[0].page_content\n",
        ")  # Print the content of the first document among the divided documents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1633cf8e",
      "metadata": {},
      "source": [
        "## Breakpoints\n",
        "This chunker works by determining when to \"split\" sentences. \n",
        "\n",
        "This is done by examining the embedding differences between two sentences.\n",
        "\n",
        "If the difference exceeds a certain threshold, the sentences are split.\n",
        "\n",
        "- Reference video: https://youtu.be/8OJC21T2SL4?si=PzUtNGYJ_KULq3-w&t=2580\n",
        "\n",
        "### Percentile\n",
        "The basic splitting method is based on `Percentile`.\n",
        "\n",
        "In this method, all differences between sentences are calculated, then splitting is done based on the specified percentile.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "744bbd95",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(\n",
        "    # Initialize the semantic chunker using OpenAI's embedding model\n",
        "    OpenAIEmbeddings(),\n",
        "    # Set the split breakpoint type to percentile\n",
        "    breakpoint_threshold_type=\"percentile\",\n",
        "    breakpoint_threshold_amount=70,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59aa8318",
      "metadata": {},
      "source": [
        "Check the split results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c7b3262",
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = text_splitter.create_documents([file])\n",
        "for i, doc in enumerate(docs[:5]):\n",
        "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
        "    print(\n",
        "        doc.page_content\n",
        "    )  # Print the content of the first document among the split documents.\n",
        "    print(\"===\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e83f74",
      "metadata": {},
      "source": [
        "Print the length of `docs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c0cbd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(docs))  # Print the length of docs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c1c9e8",
      "metadata": {},
      "source": [
        "### Standard Deviation\n",
        "\n",
        "In this method, splitting occurs when there is a difference greater than the specified `breakpoint_threshold_amount` standard deviation.\n",
        "\n",
        "- Set the `breakpoint_threshold_type` parameter to \"standard_deviation\" to specify chunk splitting criteria based on standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "16a8d823",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(\n",
        "    # Initialize the semantic chunker using OpenAI's embedding model.\n",
        "    OpenAIEmbeddings(),\n",
        "    # Use standard deviation as the splitting criterion.\n",
        "    breakpoint_threshold_type=\"standard_deviation\",\n",
        "    breakpoint_threshold_amount=1.25,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690db96c",
      "metadata": {},
      "source": [
        "Check the split results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1764de39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split using text_splitter.\n",
        "docs = text_splitter.create_documents([file])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0743d8f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = text_splitter.create_documents([file])\n",
        "for i, doc in enumerate(docs[:5]):\n",
        "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
        "    print(\n",
        "        doc.page_content\n",
        "    )  # Print the content of the first document among the split documents.\n",
        "    print(\"===\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "095170af",
      "metadata": {},
      "source": [
        "Print the length of `docs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee9f46ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(docs))  # Print the length of docs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5b03d9b",
      "metadata": {},
      "source": [
        "### Interquartile\n",
        "\n",
        "This method uses interquartile range to split chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb408177",
      "metadata": {},
      "source": [
        "- Set the `breakpoint_threshold_type` parameter to \"interquartile\" to specify chunk splitting criteria based on interquartile range.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f32f5fe8",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(\n",
        "    # Initialize the semantic chunk splitter using OpenAI's embedding model.\n",
        "    OpenAIEmbeddings(),\n",
        "    # Set the breakpoint threshold type to interquartile range.\n",
        "    breakpoint_threshold_type=\"interquartile\",\n",
        "    breakpoint_threshold_amount=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e0d2d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split using text_splitter.\n",
        "docs = text_splitter.create_documents([file])\n",
        "\n",
        "# Print the results.\n",
        "for i, doc in enumerate(docs[:5]):\n",
        "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
        "    print(\n",
        "        doc.page_content\n",
        "    )  # Print the content of the first document among the split documents.\n",
        "    print(\"===\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d186bb7",
      "metadata": {},
      "source": [
        "Print the length of `docs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c693c11",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(docs))  # Print the length of docs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-HDS-w_h3-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}