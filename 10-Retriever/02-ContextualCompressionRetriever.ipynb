{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "059756b7",
      "metadata": {},
      "source": [
        "# Contextual Compression Retriever\n",
        "\n",
        "- Author: [JoonHo Kim](https://github.com/jhboyo)\n",
        "- Design: []()\n",
        "- Peer Review :\n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/06-DocumentLoader/04-CSV-Loader.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/06-DocumentLoader/04-CSV-Loader.ipynb)\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "The `ContextualCompressionRetriever` in LangChain is a powerful tool designed to optimize the retrieval process by compressing retrieved documents based on context. This retriever is particularly useful in scenarios where large amounts of data need to be summarized or filtered dynamically, ensuring that only the most relevant information is passed to subsequent processing steps.\n",
        "\n",
        "Key features of the ContextualCompressionRetriever include:\n",
        "\n",
        "- Context-Aware Compression: Documents are compressed based on the specific context or query, ensuring relevance and reducing redundancy.\n",
        "- Flexible Integration: Works seamlessly with other LangChain components, making it easy to integrate into existing pipelines.\n",
        "- Customizable Compression: Allows for the use of different compression techniques, including summary models and embedding-based methods, to tailor the retrieval process to your needs.\n",
        "\n",
        "The `ContextualCompressionRetriever` is particularly suited for applications like:\n",
        "\n",
        "- Summarizing large datasets for Q&A systems.\n",
        "- Enhancing chatbot performance by providing concise and relevant responses.\n",
        "- Improving efficiency in document-heavy tasks like legal analysis or academic research.\n",
        "\n",
        "By using this retriever, developers can significantly reduce computational overhead and improve the quality of information presented to end-users.\n",
        "\n",
        "![](./assets/02-contextual-compression-retriever-workflow.png)  \n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Basic Retriever Configuration](#basic-retriever-configuration)\n",
        "- [Contextual Compression](#contextual-compression)\n",
        "- [Document Filtering Using LLM](#document-filtering-using-llm)\n",
        "- [Creating a Pipeline (Compressor + Document Converter)](#creating-a-pipeline-compressor--document-converter)\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "- [How to do retrieval with contextual compression](https://python.langchain.com/docs/how_to/contextual_compression/)\n",
        "- [LLM ChainFilter](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.chain_filter.LLMChainFilter.html)\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "923a83b4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "8e8ad1ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "ea1677a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langchain\",\n",
        "        \"langchain_openai\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_text_splitters\",\n",
        "        \"langchain_core\",\n",
        "        \"faiss-cpu\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "3e14778c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"Contextual Compression Retriever\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8e843e",
      "metadata": {},
      "source": [
        "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it. \n",
        "\n",
        "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "433c7da6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9da6033",
      "metadata": {},
      "source": [
        "The following function is used to display documents in a visually appealing format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "71ce609d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to print documents in a pretty format\n",
        "def pretty_print_docs(docs):\n",
        "    print(\n",
        "        f\"\\n{'-' * 100}\\n\".join(\n",
        "            [f\"document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df03b400",
      "metadata": {},
      "source": [
        "## Basic Retriever Configuration\n",
        "\n",
        "Let's start by initializing a simple vector store retriever and saving text documents in chunks.\n",
        "When a sample question is asked, you can see that the retriever returns 1 to 2 relevant documents along with a few irrelevant ones.\n",
        "\n",
        "We will follow the following steps to generate a retriever.\n",
        "1. Generate Loader to load text file using TextLoader\n",
        "2. Generate text chunks using CharacterTextSplitter and split the text into chunks of 300 characters with no overlap.\n",
        "3. Generate vector store using FAISS and convert it to retriever\n",
        "4. Query the retriever to find relevant documents\n",
        "5. Print the relevant documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "ea60830d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 380, which is longer than the specified 300\n",
            "Created a chunk of size 343, which is longer than the specified 300\n",
            "Created a chunk of size 304, which is longer than the specified 300\n",
            "Created a chunk of size 341, which is longer than the specified 300\n",
            "Created a chunk of size 349, which is longer than the specified 300\n",
            "Created a chunk of size 330, which is longer than the specified 300\n",
            "Created a chunk of size 385, which is longer than the specified 300\n",
            "Created a chunk of size 349, which is longer than the specified 300\n",
            "Created a chunk of size 413, which is longer than the specified 300\n",
            "Created a chunk of size 310, which is longer than the specified 300\n",
            "Created a chunk of size 391, which is longer than the specified 300\n",
            "Created a chunk of size 330, which is longer than the specified 300\n",
            "Created a chunk of size 325, which is longer than the specified 300\n",
            "Created a chunk of size 349, which is longer than the specified 300\n",
            "Created a chunk of size 321, which is longer than the specified 300\n",
            "Created a chunk of size 361, which is longer than the specified 300\n",
            "Created a chunk of size 437, which is longer than the specified 300\n",
            "Created a chunk of size 374, which is longer than the specified 300\n",
            "Created a chunk of size 324, which is longer than the specified 300\n",
            "Created a chunk of size 412, which is longer than the specified 300\n",
            "Created a chunk of size 346, which is longer than the specified 300\n",
            "Created a chunk of size 403, which is longer than the specified 300\n",
            "Created a chunk of size 331, which is longer than the specified 300\n",
            "Created a chunk of size 344, which is longer than the specified 300\n",
            "Created a chunk of size 350, which is longer than the specified 300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "document 1:\n",
            "\n",
            "Multimodal\n",
            "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
            "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
            "Relate\n",
            "----------------------------------------------------------------------------------------------------\n",
            "document 2:\n",
            "\n",
            "Semantic Search\n",
            "----------------------------------------------------------------------------------------------------\n",
            "document 3:\n",
            "\n",
            "LLM (Large Language Model)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "document 4:\n",
            "\n",
            "Embedding\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "# 1. Generate Loader to lthe text file using TextLoader\n",
        "loader = TextLoader(\"./data/appendix-keywords.txt\")\\\n",
        "\n",
        "# 2. Generate text chunks using CharacterTextSplitter and split the text into chunks of 300 characters with no overlap.\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
        "texts = loader.load_and_split(text_splitter)\n",
        "\n",
        "# 3. Generate vector store using FAISS and convert it to retriever\n",
        "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
        "\n",
        "# 4. Query the retriever to find relevant documents\n",
        "docs = retriever.invoke(\"What is the definition of Multimodal?\")\n",
        "\n",
        "# 5. Print the relevant documents\n",
        "pretty_print_docs(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "358f1c4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multimodal\n",
            "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
            "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
            "Relate\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2136512",
      "metadata": {},
      "source": [
        "## Contextual Compression \n",
        "\n",
        "The `DocumentCompressor` created using `LLMChainExtractor` is exactly what is applied to the retriever, which is the `ContextualCompressionRetriever`.\n",
        "\n",
        "`ContextualCompressionRetriever` will compress the documents by removing irrelevant information and focusing on the most relevant information.\n",
        "\n",
        "Let's see how the retriever works before and after applying `ContextualCompressionRetriever`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "a4b1fc3a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "document 1:\n",
            "\n",
            "Multimodal\n",
            "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
            "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
            "Relate\n",
            "----------------------------------------------------------------------------------------------------\n",
            "document 2:\n",
            "\n",
            "Semantic Search\n",
            "----------------------------------------------------------------------------------------------------\n",
            "document 3:\n",
            "\n",
            "LLM (Large Language Model)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "document 4:\n",
            "\n",
            "Embedding\n",
            "==============================================================\n",
            "===============After applying LLMChainExtractor===============\n",
            "document 1:\n",
            "\n",
            "Multimodal\n",
            "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Before applying ContextualCompressionRetriever\n",
        "pretty_print_docs(retriever.invoke(\"What is the definition of Multimodal?\"))\n",
        "print(\"=\"*62)\n",
        "print(\"=\"*15 + \"After applying LLMChainExtractor\" + \"=\"*15)\n",
        "\n",
        "\n",
        "# After applying ContextualCompressionRetriever\n",
        "# 1. Generate LLM\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")  \n",
        "\n",
        "# 2. Generate compressor using LLMChainExtractor\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "# 3. Generate compression retriever using ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=retriever,\n",
        ")\n",
        "\n",
        "# 4. Query the compression retriever to find relevant documents\n",
        "compressed_docs = (\n",
        "    compression_retriever.invoke( \n",
        "        \"What is the definition of Multimodal?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# 5. Print the relevant documents\n",
        "pretty_print_docs(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74acacb3",
      "metadata": {},
      "source": [
        "## Document Filtering Using LLM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdc45983",
      "metadata": {},
      "source": [
        "### LLMChainFilter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31c71c0d",
      "metadata": {},
      "source": [
        "`LLMChainFilter` is a simpler yet powerful compressor that uses an LLM chain to decide which documents to filter and which to return from the initially retrieved documents. \n",
        "\n",
        "This filter selectively returns documents without altering (compressing) their content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "0b4f08a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "document 1:\n",
            "\n",
            "Multimodal\n",
            "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
            "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
            "Relate\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers.document_compressors import LLMChainFilter\n",
        "\n",
        "# 1. Generate LLMChainFilter object using LLM\n",
        "_filter = LLMChainFilter.from_llm(llm)\n",
        "\n",
        "# 2. Generate ContextualCompressionRetriever object using LLMChainFilter and retriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=_filter,\n",
        "    base_retriever=retriever,\n",
        ")\n",
        "\n",
        "# 3. Query the compression retriever to find relevant documents\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    \"What is the definition of Multimodal?\"\n",
        ")\n",
        "\n",
        "# 4. Print the relevant documents\n",
        "pretty_print_docs(compressed_docs)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3de64ab",
      "metadata": {},
      "source": [
        "### EmbeddingsFilter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a246efd6",
      "metadata": {},
      "source": [
        "Performing additional LLM calls for each retrieved document is costly and slow. \n",
        "The `EmbeddingsFilter` provides a more affordable and faster option by embedding both the documents and the query, returning only those documents with embeddings that are sufficiently similar to the query. \n",
        "\n",
        "This allows for maintaining the relevance of search results while saving on computational costs and time.\n",
        "The process involves compressing and retrieving relevant documents using `EmbeddingsFilter` and `ContextualCompressionRetriever`. \n",
        "\n",
        "- The `EmbeddingsFilter` is used to filter documents that exceed a specified similarity threshold (0.86)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "106461e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "document 1:\n",
            "\n",
            "Multimodal\n",
            "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
            "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
            "Relate\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# 1. Generate embeddings using OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# 2. Generate EmbedingsFilter object that has similarity threshold of 0.86\n",
        "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)\n",
        "\n",
        "# 3. Generate ContextualCompressionRetriever object using EmbeddingsFilter and retriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=embeddings_filter, \n",
        "    base_retriever=retriever\n",
        ")\n",
        "\n",
        "# 4. Query the compression retriever to find relevant documents\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    \"What is the definition of Multimodal?\"\n",
        ")\n",
        "\n",
        "# 5. Print the relevant documents\n",
        "pretty_print_docs(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb1381a0",
      "metadata": {},
      "source": [
        "## Creating a Pipeline (Compressor + Document Converter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dca7126d",
      "metadata": {},
      "source": [
        "Using `DocumentCompressorPipeline`, multiple compressors can be sequentially combined.\n",
        "\n",
        "You can add `BaseDocumentTransformer` to the pipeline along with the Compressor, which performs transformations on the document set without performing contextual compression.\n",
        "\n",
        "For example, `TextSplitter` can be used as a document transformer to split documents into smaller pieces, while `EmbeddingsRedundantFilter` can be used to filter out duplicate documents based on the embedding similarity between documents (by default, considering documents with a similarity of 0.95 or higher as duplicates).\n",
        "\n",
        "Below, we first split the documents into smaller chunks, then remove duplicate documents, and filter based on relevance to the query to create a compressor pipeline.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "91a64cb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "# 1. Generate CharacterTextSplitter object that has chunk size of 300 and chunk overlap of 0\n",
        "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
        "\n",
        "# 2. Generate EmbeddingsRedundantFilter object using embeddings\n",
        "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
        "\n",
        "# 3. Generate EmbeddingsFilter object that has similarity threshold of 0.86\n",
        "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)\n",
        "\n",
        "# 4. Generate DocumentCompressorPipeline object using splitter, redundant_filter, relevant_filter, and LLMChainExtractor\n",
        "pipeline_compressor = DocumentCompressorPipeline(\n",
        "    transformers=[\n",
        "        splitter,\n",
        "        redundant_filter,\n",
        "        relevant_filter,\n",
        "        LLMChainExtractor.from_llm(llm),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f935e0ad",
      "metadata": {},
      "source": [
        "While initializing the  `ContextualCompressionRetriever`, we use `pipeline_compressor` as the `base_compressor` and `retriever` as the `base_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "1da48dc6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "document 1:\n",
            "\n",
            "Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n"
          ]
        }
      ],
      "source": [
        "# 5. Use pipeline_compressor as the base_compressor and retriever as the base_retriever to initialize ContextualCompressionRetriever\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=pipeline_compressor,\n",
        "    base_retriever=retriever,\n",
        ")\n",
        "\n",
        "# 6. Query the compression retriever to find relevant documents\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    \"What is the definition of Multimodal?\"\n",
        ")\n",
        "\n",
        "# 7. Print the relevant documents\n",
        "pretty_print_docs(compressed_docs)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py-test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
