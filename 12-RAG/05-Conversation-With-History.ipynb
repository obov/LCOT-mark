{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635d8ebb",
   "metadata": {},
   "source": [
    "# Conversation-With-History\n",
    "\n",
    "- Author: [Sunworl Kim](https://github.com/sunworl)\n",
    "- Design:\n",
    "- Peer Review:\n",
    "- Proofread:\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial provides a comprehensive guide to implementing **conversational AI systems** with memory capabilities using LangChain in two main approaches.\n",
    "\n",
    "**1. Creating a chain to record conversations**\n",
    "\n",
    "- Creates a simple question-answering **chatbot** using ChatOpenAI.\n",
    "\n",
    "- Implements a system to store and retrieve conversation history based on session IDs.\n",
    "\n",
    "- Uses **RunnableWithMessageHistory** to incorporate chat history into the chain.\n",
    "\n",
    "\n",
    "**2. Creating a RAG chain that retrieves information from documents and records conversations**\n",
    "\n",
    "- Builds a more complex system that combines document retrieval with conversational AI. \n",
    "\n",
    "- Processes a **PDF document**, creates embeddings, and sets up a vector store for efficient retrieval.\n",
    "\n",
    "- Implements a **RAG chain** that can answer questions based on the document content and previous conversation history.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Creating a Chain that remembers previous conversations](#creating-a-chain-that-remembers-previous-conversations)\n",
    "  - [1. Add conversation history to the general Chain](#1-add-conversation-history-to-the-general-chain)\n",
    "  - [2. RAG + RunnableWithMessageHistory](#2-rag--runnablewithmessagehistory)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- [Langchain Python API : RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)\n",
    "- [Langchain docs : Build a Chatbot](https://python.langchain.com/docs/tutorials/chatbot/) \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7aba4",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21943adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f25ec196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_openai\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f9065ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Conversation-With-History\"  \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a9ae0",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f99b5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00c3f4",
   "metadata": {},
   "source": [
    "## Creating a Chain that remembers previous conversations\n",
    "\n",
    "Background knowledge needed to understand this content : [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#runnablewithmessagehistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fc536",
   "metadata": {},
   "source": [
    "## 1. Add conversation history to the general Chain\n",
    "\n",
    "- Use `MessagesPlaceholder` to include conversation history.\n",
    "\n",
    "- Define a prompt that takes user input for questions.\n",
    "\n",
    "- Create a `ChatOpenAI` instance that uses OpenAI's `ChatGPT` model.\n",
    "\n",
    "- Build a chain by connecting the prompt, language model, and output parser.\n",
    "\n",
    "- Use `StrOutputParser` to convert the model's output into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b78d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Defining the prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a Question-Answering chatbot. Please provide an answer to the given question.\",\n",
    "        ),\n",
    "        # Please use the key 'chat_history' for conversation history without changing it if possible!\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"#Question:\\n{question}\"),  # Use user input as a variable\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generating an LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Creating a regular Chain\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4d831",
   "metadata": {},
   "source": [
    "Creating a chain that records conversations (chain_with_history)\n",
    "\n",
    "- Create a dictionary to store session records.\n",
    "\n",
    "- Define a function to retrieve session records based on session ID. If the session ID is not in the store, create a new `ChatMessageHistory` object.\n",
    "\n",
    "- Create a `RunnableWithMessageHistory` object to manage conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0874c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store session records\n",
    "store = {}\n",
    "\n",
    "# Function to retrieve session records based on session ID\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[Conversation Session ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # If the session ID is not in the store\n",
    "        # Create a new ChatMessageHistory object and save it to the store\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # Return the session history for the corresponding session ID\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # Function to retrieve session history\n",
    "    input_messages_key=\"question\",  # Key for the template variable that will contain the user's question\n",
    "    history_messages_key=\"chat_history\",  # Key for the history messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c108df",
   "metadata": {},
   "source": [
    "Execute the first question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2d22b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conversation Session ID]: abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Jack! How can I help you today?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    # Input question\n",
    "    {\"question\": \"My name is Jack.\"},\n",
    "    # Record the conversation based on the session ID.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0901c",
   "metadata": {},
   "source": [
    "Execute the question in continuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec89414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conversation Session ID]: abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Jack.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    # Input question\n",
    "    {\"question\": \"What is my name?\"},\n",
    "    # Record the conversation based on the session ID.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc43b99",
   "metadata": {},
   "source": [
    "## 2. RAG + RunnableWithMessageHistory\n",
    "\n",
    "Implement a PDF document-based question-answering (QA) system.\n",
    "\n",
    "First, create a regular RAG Chain, However, make sure to include `{chat_history}` in the prompt for step 6.\n",
    "\n",
    "- (step 1) Use `PDFPlumberLoader` to load PDF files.\n",
    "\n",
    "- (step 2)  Split documents into smaller chunks using `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "- (step 3)  Generate vector representations of text chunks using `OpenAIEmbeddings`.\n",
    "\n",
    "- (step 4)  Store embeddings and make them searchable using `FAISS`.\n",
    "\n",
    "- (step 5) Create a `retriever` to search for relevant information in the vector database.\n",
    "\n",
    "- (step 6)  Generate a prompt template for question-answering tasks, including previous conversation history, questions, and context, with instructions to answer.\n",
    "\n",
    "- (step 7)  Initialize the `GPT-4o` model using `ChatOpenAI`.\n",
    "\n",
    "- (step 8)  Construct a chain that connects retrieval, prompt processing, and language model inference.\n",
    "\n",
    "Retrieve relevant context for user questions and generate answers based on this context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1221d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from operator import itemgetter\n",
    "\n",
    "# Step 1: Load Documents\n",
    "loader = PDFPlumberLoader(\"data/A European Approach to Artificial Intelligence - A Policy Perspective.pdf\") \n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 3: Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Step 4: Create DB and Save\n",
    "# Create the vector store.\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# Step 5: Create Retriever\n",
    "# Retrieve and generate information contained in the documents.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Step 6: Create Prompt\n",
    "# Generate the prompt.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know.\n",
    "\n",
    "#Previous Chat History:\n",
    "{chat_history}\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Step 7: Create Language Model (LLM)\n",
    "# Generate the model (LLM).\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Step 8: Create Chain\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927cac10",
   "metadata": {},
   "source": [
    "Defining a function to save the conversation.\n",
    "\n",
    "- The `store` dictionary is used to save conversation histories according to `session ids`, and the `get_session_history` function retrieves session records. \n",
    "\n",
    "- A `RunnableWithMessageHistory` object is created to add conversation history management functionality to the `RAG chain`, processing user questions and conversation histories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fa5e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store session records\n",
    "store = {}\n",
    "\n",
    "# Function to retrieve session records based on session ID\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[Conversation Session ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # If the session ID is not in the store\n",
    "        # Create a new ChatMessageHistory object and save it to the store\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  \n",
    "\n",
    "# Create a RAG chain that records conversations\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # Function to retrieve session history\n",
    "    input_messages_key=\"question\",  # Key for the template variable that will contain the user's question\n",
    "    history_messages_key=\"chat_history\",  # Key for the history messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2753835",
   "metadata": {},
   "source": [
    "Execute the first question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef397b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conversation Session ID]: rag123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The three key components necessary to achieve 'trustworthy AI' in the European approach to AI policy are: (1) compliance with the law, (2) fulfillment of ethical principles, and (3) robustness.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_history.invoke(\n",
    "    # Input question\n",
    "    {\"question\": \"What are the three key components necessary to achieve 'trustworthy AI' in the European approach to AI policy?\"},\n",
    "    # Record the conversation based on the session ID.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15026fb6",
   "metadata": {},
   "source": [
    "Execute the subsequent question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11c37944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conversation Session ID]: rag123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Los tres componentes clave necesarios para lograr una \"IA confiable\" en el enfoque europeo de la política de IA son: (1) cumplimiento de la ley, (2) cumplimiento de principios éticos y (3) robustez.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_history.invoke(\n",
    "    # Input question\n",
    "    {\"question\": \"Please translate the previous answer into Spanish.\"},\n",
    "    # Record the conversation based on the session ID.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
